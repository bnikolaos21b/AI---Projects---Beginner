{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd4d2954-936e-4086-b2c0-88c3dc36fce9",
   "metadata": {},
   "source": [
    "# House Prices Prediction - Regression Techniques\n",
    "\n",
    "My name is **Nikos**, and this project along with others in this folder mark a significant milestone in my journey to becoming an AI Engineer. I’m passionate about artificial intelligence and machine learning, and this project serves as the first step in that path. Through it, I aim to not only understand how neural networks work but also demonstrate the practical applications of AI in real-world problems.\n",
    "\n",
    "In this project, we aim to predict house prices using various features available in the dataset. We will explore the dataset, clean the data, perform feature engineering, and then train machine learning models to make predictions.\n",
    "\n",
    "The steps we will follow:\n",
    "1. Data Exploration\n",
    "2. Data Cleaning\n",
    "3. Feature Engineering\n",
    "4. Model Selection\n",
    "5. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2297ee-11b6-489e-909a-81dcdc31cd98",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "In this step, we load the dataset and explore its structure, checking for missing values, data types, and summary statistics. Understanding the data is crucial before performing any cleaning or preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db22137a-27a4-455c-a51a-1ecd6c9cbbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0759451c-3f6b-4b3e-9c4a-bd052ad2f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test data\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7585a195-921a-4696-ba36-d64354b882da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the training data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d7b78-8afc-4ff8-aa76-8e1129ab526e",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "\n",
    "Before diving into machine learning, let's explore the dataset. We will check the size of the dataset, look at the data types, and identify any potential issues such as missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ae04aa-491a-49f1-a8d2-a9b09e28522e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 81 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Id             1460 non-null   int64  \n",
      " 1   MSSubClass     1460 non-null   int64  \n",
      " 2   MSZoning       1460 non-null   object \n",
      " 3   LotFrontage    1201 non-null   float64\n",
      " 4   LotArea        1460 non-null   int64  \n",
      " 5   Street         1460 non-null   object \n",
      " 6   Alley          91 non-null     object \n",
      " 7   LotShape       1460 non-null   object \n",
      " 8   LandContour    1460 non-null   object \n",
      " 9   Utilities      1460 non-null   object \n",
      " 10  LotConfig      1460 non-null   object \n",
      " 11  LandSlope      1460 non-null   object \n",
      " 12  Neighborhood   1460 non-null   object \n",
      " 13  Condition1     1460 non-null   object \n",
      " 14  Condition2     1460 non-null   object \n",
      " 15  BldgType       1460 non-null   object \n",
      " 16  HouseStyle     1460 non-null   object \n",
      " 17  OverallQual    1460 non-null   int64  \n",
      " 18  OverallCond    1460 non-null   int64  \n",
      " 19  YearBuilt      1460 non-null   int64  \n",
      " 20  YearRemodAdd   1460 non-null   int64  \n",
      " 21  RoofStyle      1460 non-null   object \n",
      " 22  RoofMatl       1460 non-null   object \n",
      " 23  Exterior1st    1460 non-null   object \n",
      " 24  Exterior2nd    1460 non-null   object \n",
      " 25  MasVnrType     588 non-null    object \n",
      " 26  MasVnrArea     1452 non-null   float64\n",
      " 27  ExterQual      1460 non-null   object \n",
      " 28  ExterCond      1460 non-null   object \n",
      " 29  Foundation     1460 non-null   object \n",
      " 30  BsmtQual       1423 non-null   object \n",
      " 31  BsmtCond       1423 non-null   object \n",
      " 32  BsmtExposure   1422 non-null   object \n",
      " 33  BsmtFinType1   1423 non-null   object \n",
      " 34  BsmtFinSF1     1460 non-null   int64  \n",
      " 35  BsmtFinType2   1422 non-null   object \n",
      " 36  BsmtFinSF2     1460 non-null   int64  \n",
      " 37  BsmtUnfSF      1460 non-null   int64  \n",
      " 38  TotalBsmtSF    1460 non-null   int64  \n",
      " 39  Heating        1460 non-null   object \n",
      " 40  HeatingQC      1460 non-null   object \n",
      " 41  CentralAir     1460 non-null   object \n",
      " 42  Electrical     1459 non-null   object \n",
      " 43  1stFlrSF       1460 non-null   int64  \n",
      " 44  2ndFlrSF       1460 non-null   int64  \n",
      " 45  LowQualFinSF   1460 non-null   int64  \n",
      " 46  GrLivArea      1460 non-null   int64  \n",
      " 47  BsmtFullBath   1460 non-null   int64  \n",
      " 48  BsmtHalfBath   1460 non-null   int64  \n",
      " 49  FullBath       1460 non-null   int64  \n",
      " 50  HalfBath       1460 non-null   int64  \n",
      " 51  BedroomAbvGr   1460 non-null   int64  \n",
      " 52  KitchenAbvGr   1460 non-null   int64  \n",
      " 53  KitchenQual    1460 non-null   object \n",
      " 54  TotRmsAbvGrd   1460 non-null   int64  \n",
      " 55  Functional     1460 non-null   object \n",
      " 56  Fireplaces     1460 non-null   int64  \n",
      " 57  FireplaceQu    770 non-null    object \n",
      " 58  GarageType     1379 non-null   object \n",
      " 59  GarageYrBlt    1379 non-null   float64\n",
      " 60  GarageFinish   1379 non-null   object \n",
      " 61  GarageCars     1460 non-null   int64  \n",
      " 62  GarageArea     1460 non-null   int64  \n",
      " 63  GarageQual     1379 non-null   object \n",
      " 64  GarageCond     1379 non-null   object \n",
      " 65  PavedDrive     1460 non-null   object \n",
      " 66  WoodDeckSF     1460 non-null   int64  \n",
      " 67  OpenPorchSF    1460 non-null   int64  \n",
      " 68  EnclosedPorch  1460 non-null   int64  \n",
      " 69  3SsnPorch      1460 non-null   int64  \n",
      " 70  ScreenPorch    1460 non-null   int64  \n",
      " 71  PoolArea       1460 non-null   int64  \n",
      " 72  PoolQC         7 non-null      object \n",
      " 73  Fence          281 non-null    object \n",
      " 74  MiscFeature    54 non-null     object \n",
      " 75  MiscVal        1460 non-null   int64  \n",
      " 76  MoSold         1460 non-null   int64  \n",
      " 77  YrSold         1460 non-null   int64  \n",
      " 78  SaleType       1460 non-null   object \n",
      " 79  SaleCondition  1460 non-null   object \n",
      " 80  SalePrice      1460 non-null   int64  \n",
      "dtypes: float64(3), int64(35), object(43)\n",
      "memory usage: 924.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1452.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>730.500000</td>\n",
       "      <td>56.897260</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>10516.828082</td>\n",
       "      <td>6.099315</td>\n",
       "      <td>5.575342</td>\n",
       "      <td>1971.267808</td>\n",
       "      <td>1984.865753</td>\n",
       "      <td>103.685262</td>\n",
       "      <td>443.639726</td>\n",
       "      <td>...</td>\n",
       "      <td>94.244521</td>\n",
       "      <td>46.660274</td>\n",
       "      <td>21.954110</td>\n",
       "      <td>3.409589</td>\n",
       "      <td>15.060959</td>\n",
       "      <td>2.758904</td>\n",
       "      <td>43.489041</td>\n",
       "      <td>6.321918</td>\n",
       "      <td>2007.815753</td>\n",
       "      <td>180921.195890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>421.610009</td>\n",
       "      <td>42.300571</td>\n",
       "      <td>24.284752</td>\n",
       "      <td>9981.264932</td>\n",
       "      <td>1.382997</td>\n",
       "      <td>1.112799</td>\n",
       "      <td>30.202904</td>\n",
       "      <td>20.645407</td>\n",
       "      <td>181.066207</td>\n",
       "      <td>456.098091</td>\n",
       "      <td>...</td>\n",
       "      <td>125.338794</td>\n",
       "      <td>66.256028</td>\n",
       "      <td>61.119149</td>\n",
       "      <td>29.317331</td>\n",
       "      <td>55.757415</td>\n",
       "      <td>40.177307</td>\n",
       "      <td>496.123024</td>\n",
       "      <td>2.703626</td>\n",
       "      <td>1.328095</td>\n",
       "      <td>79442.502883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "      <td>34900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>365.750000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>7553.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>1967.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>129975.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>730.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>9478.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>383.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>163000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1095.250000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>11601.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>712.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>214000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>5644.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>857.000000</td>\n",
       "      <td>547.000000</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>15500.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>755000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id   MSSubClass  LotFrontage        LotArea  OverallQual  \\\n",
       "count  1460.000000  1460.000000  1201.000000    1460.000000  1460.000000   \n",
       "mean    730.500000    56.897260    70.049958   10516.828082     6.099315   \n",
       "std     421.610009    42.300571    24.284752    9981.264932     1.382997   \n",
       "min       1.000000    20.000000    21.000000    1300.000000     1.000000   \n",
       "25%     365.750000    20.000000    59.000000    7553.500000     5.000000   \n",
       "50%     730.500000    50.000000    69.000000    9478.500000     6.000000   \n",
       "75%    1095.250000    70.000000    80.000000   11601.500000     7.000000   \n",
       "max    1460.000000   190.000000   313.000000  215245.000000    10.000000   \n",
       "\n",
       "       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  ...  \\\n",
       "count  1460.000000  1460.000000   1460.000000  1452.000000  1460.000000  ...   \n",
       "mean      5.575342  1971.267808   1984.865753   103.685262   443.639726  ...   \n",
       "std       1.112799    30.202904     20.645407   181.066207   456.098091  ...   \n",
       "min       1.000000  1872.000000   1950.000000     0.000000     0.000000  ...   \n",
       "25%       5.000000  1954.000000   1967.000000     0.000000     0.000000  ...   \n",
       "50%       5.000000  1973.000000   1994.000000     0.000000   383.500000  ...   \n",
       "75%       6.000000  2000.000000   2004.000000   166.000000   712.250000  ...   \n",
       "max       9.000000  2010.000000   2010.000000  1600.000000  5644.000000  ...   \n",
       "\n",
       "        WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  ScreenPorch  \\\n",
       "count  1460.000000  1460.000000    1460.000000  1460.000000  1460.000000   \n",
       "mean     94.244521    46.660274      21.954110     3.409589    15.060959   \n",
       "std     125.338794    66.256028      61.119149    29.317331    55.757415   \n",
       "min       0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "50%       0.000000    25.000000       0.000000     0.000000     0.000000   \n",
       "75%     168.000000    68.000000       0.000000     0.000000     0.000000   \n",
       "max     857.000000   547.000000     552.000000   508.000000   480.000000   \n",
       "\n",
       "          PoolArea       MiscVal       MoSold       YrSold      SalePrice  \n",
       "count  1460.000000   1460.000000  1460.000000  1460.000000    1460.000000  \n",
       "mean      2.758904     43.489041     6.321918  2007.815753  180921.195890  \n",
       "std      40.177307    496.123024     2.703626     1.328095   79442.502883  \n",
       "min       0.000000      0.000000     1.000000  2006.000000   34900.000000  \n",
       "25%       0.000000      0.000000     5.000000  2007.000000  129975.000000  \n",
       "50%       0.000000      0.000000     6.000000  2008.000000  163000.000000  \n",
       "75%       0.000000      0.000000     8.000000  2009.000000  214000.000000  \n",
       "max     738.000000  15500.000000    12.000000  2010.000000  755000.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the shape of the training dataset\n",
    "train_df.shape\n",
    "\n",
    "# Check the data types and non-null counts for each column\n",
    "train_df.info()\n",
    "\n",
    "# Display summary statistics of the numerical columns\n",
    "train_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d88f04e8-82db-45f2-b5d1-b69d7d99138e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                 0\n",
       "MSSubClass         0\n",
       "MSZoning           0\n",
       "LotFrontage      259\n",
       "LotArea            0\n",
       "                ... \n",
       "MoSold             0\n",
       "YrSold             0\n",
       "SaleType           0\n",
       "SaleCondition      0\n",
       "SalePrice          0\n",
       "Length: 81, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79251bbd-0b7b-4ea8-9c36-6de626115745",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "\n",
    "Based on the missing value analysis, `LotFrontage` has 259 missing values. Since it’s a numerical feature, we will fill missing values using the **median** value, as this is a common approach when dealing with skewed data.\n",
    "\n",
    "Step 3.1: Filling Missing Values in LotFrontage\n",
    "\n",
    "To handle the \"SettingWithCopyWarning\", we will avoid using `inplace=True` and instead reassign the column directly after filling the missing values with the median.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f7f88e-74c0-4baa-936d-bf8bc025265f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PoolQC          1453\n",
       "MiscFeature     1406\n",
       "Alley           1369\n",
       "Fence           1179\n",
       "MasVnrType       872\n",
       "FireplaceQu      690\n",
       "GarageCond        81\n",
       "GarageType        81\n",
       "GarageYrBlt       81\n",
       "GarageFinish      81\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill missing values in LotFrontage with the median\n",
    "train_df['LotFrontage'] = train_df['LotFrontage'].fillna(train_df['LotFrontage'].median())\n",
    "\n",
    "# Check if missing values in LotFrontage are filled\n",
    "train_df.isnull().sum().sort_values(ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dd7dec-e33a-44b8-84ec-c2f4ff10fc03",
   "metadata": {},
   "source": [
    "## Filling Missing Values in Categorical Columns (Remaining)\n",
    "\n",
    "The remaining categorical columns with missing values are: `PoolQC`, `MiscFeature`, `Alley`, `Fence`, `MasVnrType`, `FireplaceQu`, `GarageType`, `GarageQual`, and `GarageCond`. We'll fill the missing values in these columns with `'None'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaddda42-560c-4d96-a3aa-ee809f0cadf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GarageYrBlt     81\n",
       "GarageFinish    81\n",
       "BsmtExposure    38\n",
       "BsmtFinType2    38\n",
       "BsmtQual        37\n",
       "BsmtCond        37\n",
       "BsmtFinType1    37\n",
       "MasVnrArea       8\n",
       "Electrical       1\n",
       "HalfBath         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of categorical features with missing values\n",
    "categorical_features = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'MasVnrType', \n",
    "                        'FireplaceQu', 'GarageType', 'GarageQual', 'GarageCond']\n",
    "\n",
    "# Fill missing values in these categorical columns with 'None'\n",
    "train_df[categorical_features] = train_df[categorical_features].fillna('None')\n",
    "\n",
    "# Check if missing values are filled for these columns\n",
    "train_df.isnull().sum().sort_values(ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190a6798-fd97-4b87-aa57-89fd1640e218",
   "metadata": {},
   "source": [
    "## Filling Missing Values in Numerical Columns\n",
    "\n",
    "We will fill missing values in `GarageYrBlt` and `MasVnrArea` using the median, as these are numerical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00c6e489-af02-4356-b6f2-2096ca85aaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GarageFinish    81\n",
       "BsmtFinType2    38\n",
       "BsmtExposure    38\n",
       "BsmtFinType1    37\n",
       "BsmtCond        37\n",
       "BsmtQual        37\n",
       "Electrical       1\n",
       "Fireplaces       0\n",
       "Functional       0\n",
       "TotRmsAbvGrd     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill missing values in numerical columns with the median\n",
    "train_df['GarageYrBlt'] = train_df['GarageYrBlt'].fillna(train_df['GarageYrBlt'].median())\n",
    "train_df['MasVnrArea'] = train_df['MasVnrArea'].fillna(train_df['MasVnrArea'].median())\n",
    "\n",
    "# Check if missing values are filled for numerical columns\n",
    "train_df.isnull().sum().sort_values(ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b0de0-e87b-4d81-ab8d-463320e1649f",
   "metadata": {},
   "source": [
    "## Filling Missing Values in Categorical Columns\n",
    "\n",
    "We'll fill the remaining categorical columns (`GarageFinish`, `BsmtExposure`, `BsmtQual`, etc.) with `'None'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "615a75d0-cb89-4006-9f70-ca56a5f7fd4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Electrical      1\n",
       "GarageYrBlt     0\n",
       "GarageType      0\n",
       "FireplaceQu     0\n",
       "Fireplaces      0\n",
       "Functional      0\n",
       "TotRmsAbvGrd    0\n",
       "KitchenQual     0\n",
       "KitchenAbvGr    0\n",
       "BedroomAbvGr    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of basement-related categorical features with missing values\n",
    "basement_features = ['BsmtExposure', 'BsmtFinType2', 'BsmtQual', 'BsmtCond', 'BsmtFinType1']\n",
    "\n",
    "# Fill missing values in the categorical columns with 'None'\n",
    "train_df[basement_features + ['GarageFinish']] = train_df[basement_features + ['GarageFinish']].fillna('None')\n",
    "\n",
    "# Check if missing values are filled\n",
    "train_df.isnull().sum().sort_values(ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea0eada-5544-4f3e-b6a8-fdd79a7dc264",
   "metadata": {},
   "source": [
    "## Filling Missing Value in `Electrical`\n",
    "\n",
    "We will fill the single missing value in `Electrical` with the most common value (mode).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebd13aef-1b9e-4d3d-b2cc-70390d5e03cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SalePrice        0\n",
       "PavedDrive       0\n",
       "WoodDeckSF       0\n",
       "OpenPorchSF      0\n",
       "EnclosedPorch    0\n",
       "3SsnPorch        0\n",
       "ScreenPorch      0\n",
       "PoolArea         0\n",
       "PoolQC           0\n",
       "Utilities        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill missing value in Electrical with the most common value (mode)\n",
    "train_df['Electrical'] = train_df['Electrical'].fillna(train_df['Electrical'].mode()[0])\n",
    "\n",
    "# Check if any missing values remain\n",
    "train_df.isnull().sum().sort_values(ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00f2f2d-d621-4485-baee-5d5a2c571097",
   "metadata": {},
   "source": [
    "## Why Weren’t These Missing Values Handled Initially?\n",
    "\n",
    "In the earlier steps, we addressed missing values in some of the categorical features, but certain features related to the garage and basement were not included in our first round of handling missing values. This happened because we focused on a subset of categorical columns, and some columns (like `GarageYrBlt` and `Basement`-related features) appeared after we handled the initial group.\n",
    "\n",
    "Additionally:\n",
    "- **GarageYrBlt** is a numerical column that needed a different treatment (median filling), which wasn't included in the initial categorical columns.\n",
    "- The basement-related features (`BsmtExposure`, `BsmtQual`, etc.) also required special attention, as missing values here likely indicate that the house does not have a basement, hence filling them with `'None'`.\n",
    "\n",
    "We addressed these separately to ensure all missing values were properly handled before moving on to the next steps in the project.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps: Encoding Categorical Variables\n",
    "\n",
    "Now that we’ve handled all missing values, we need to convert categorical variables into a format that machine learning algorithms can work with. We will use **One-Hot Encoding** to transform these categorical features into binary columns.\n",
    "\n",
    "One-hot encoding is necessary because machine learning models work best with numerical data, and this step allows us to convert categories into numbers without introducing any unintended ordinal relationships between them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abc8745-16be-4713-bbce-87f5b70c56b8",
   "metadata": {},
   "source": [
    "## Encoding Categorical Variables\n",
    "\n",
    "We need to convert the categorical variables into numeric format using **One-Hot Encoding**. This will create binary columns for each category, allowing machine learning algorithms to interpret them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eaa3333-1057-42f0-a8eb-40e0cc228fef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 304 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0   1          60         65.0     8450            7            5       2003   \n",
       "1   2          20         80.0     9600            6            8       1976   \n",
       "2   3          60         68.0    11250            7            5       2001   \n",
       "3   4          70         60.0     9550            7            5       1915   \n",
       "4   5          60         84.0    14260            8            5       2000   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  ...  SaleType_ConLw  SaleType_New  \\\n",
       "0          2003       196.0         706  ...           False         False   \n",
       "1          1976         0.0         978  ...           False         False   \n",
       "2          2002       162.0         486  ...           False         False   \n",
       "3          1970         0.0         216  ...           False         False   \n",
       "4          2000       350.0         655  ...           False         False   \n",
       "\n",
       "   SaleType_Oth  SaleType_WD  SaleCondition_Abnorml  SaleCondition_AdjLand  \\\n",
       "0         False         True                  False                  False   \n",
       "1         False         True                  False                  False   \n",
       "2         False         True                  False                  False   \n",
       "3         False         True                   True                  False   \n",
       "4         False         True                  False                  False   \n",
       "\n",
       "   SaleCondition_Alloca  SaleCondition_Family  SaleCondition_Normal  \\\n",
       "0                 False                 False                  True   \n",
       "1                 False                 False                  True   \n",
       "2                 False                 False                  True   \n",
       "3                 False                 False                 False   \n",
       "4                 False                 False                  True   \n",
       "\n",
       "   SaleCondition_Partial  \n",
       "0                  False  \n",
       "1                  False  \n",
       "2                  False  \n",
       "3                  False  \n",
       "4                  False  \n",
       "\n",
       "[5 rows x 304 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply one-hot encoding to categorical variables\n",
    "train_df = pd.get_dummies(train_df)\n",
    "\n",
    "# Display the first few rows after encoding\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a3f753-e159-409d-94e7-703a3b26d685",
   "metadata": {},
   "source": [
    "## Feature Selection and Target Variable\n",
    "\n",
    "Now that all the categorical variables are encoded, it's time to separate the **features** (independent variables) from the **target variable** (`SalePrice`). \n",
    "- **X** will represent the features (all columns except `SalePrice`).\n",
    "- **y** will represent the target variable (`SalePrice`), which we want to predict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1a532a4-c618-4c19-9203-8d690347a9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1460, 303), (1460,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define feature matrix (X) and target vector (y)\n",
    "X = train_df.drop('SalePrice', axis=1)\n",
    "y = train_df['SalePrice']\n",
    "\n",
    "# Display the shapes of X and y\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a879f03-07dc-4720-b27e-18fd3ba96782",
   "metadata": {},
   "source": [
    "## Splitting the Data into Training and Validation Sets\n",
    "\n",
    "We’ll split the dataset into training and validation sets. Typically, we use 80% of the data for training and 20% for validation to ensure we can evaluate the model’s performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "190dd0d6-b67e-431e-89f4-a4aeb5ede4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1168, 303), (292, 303), (1168,), (292,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the split data\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a105c23-b35e-4c40-8e85-3b6155695501",
   "metadata": {},
   "source": [
    "## Training a Linear Regression Model\n",
    "\n",
    "We’ll start by training a **Linear Regression** model. This model is a good starting point for regression tasks like predicting house prices. After training, we’ll evaluate the model’s performance on the validation set using the **Mean Absolute Error (MAE)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2879a0b-862d-4ac9-a2e0-34d9369fe1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 21109.70645342619\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Calculate the Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "print(f'Mean Absolute Error: {mae}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba66e37e-b35a-4a96-8468-ec79bec1b9a4",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "We will create new features based on domain knowledge, which might improve the model’s ability to make accurate predictions. We will:\n",
    "1. Create an `Age` feature, which represents the age of the house.\n",
    "2. Transform the skewed `LotArea` feature using a logarithmic scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "605584b1-1fd6-44f3-98e0-279871de69f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>RemodAge</th>\n",
       "      <th>LogLotArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>9.042040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>9.169623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>9.328212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>109</td>\n",
       "      <td>54</td>\n",
       "      <td>9.164401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>9.565284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  RemodAge  LogLotArea\n",
       "0   21        21    9.042040\n",
       "1   48        48    9.169623\n",
       "2   23        22    9.328212\n",
       "3  109        54    9.164401\n",
       "4   24        24    9.565284"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a new feature 'Age' (current year - year built)\n",
    "train_df['Age'] = 2024 - train_df['YearBuilt']  # Assuming the current year is 2024\n",
    "\n",
    "# Create a new feature 'RemodAge' (current year - year remodeled)\n",
    "train_df['RemodAge'] = 2024 - train_df['YearRemodAdd']\n",
    "\n",
    "# Log-transform the 'LotArea' feature to deal with skewness\n",
    "train_df['LogLotArea'] = np.log1p(train_df['LotArea'])\n",
    "\n",
    "# Drop 'YearBuilt' and 'YearRemodAdd' as they are now represented by new features\n",
    "train_df.drop(['YearBuilt', 'YearRemodAdd'], axis=1, inplace=True)\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "train_df[['Age', 'RemodAge', 'LogLotArea']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "155f5830-58a1-4d2f-bbed-feeb783ac309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 21109.70645342619\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Calculate the Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "print(f'Mean Absolute Error: {mae}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aea47a-c355-4396-82df-d97cbd51068e",
   "metadata": {},
   "source": [
    "## No improvement from feature engineering\n",
    "\n",
    "In linear regression and not complex cases, feature engineering might not give what we want. Same is with hypertuning. So we move on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386f5ad8-d461-4097-833f-46d784fda06d",
   "metadata": {},
   "source": [
    "## Training a Decision Tree Model\n",
    "\n",
    "We will now train a **Decision Tree** model, which is more flexible and can capture non-linear relationships in the data. Decision Trees are also less sensitive to feature scaling and work well with both numerical and categorical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a492a35-addd-431c-a242-ea61f456aaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (Decision Tree): 27587.36301369863\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Initialize the Decision Tree model\n",
    "tree_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred_tree = tree_model.predict(X_val)\n",
    "\n",
    "# Calculate the Mean Absolute Error (MAE)\n",
    "mae_tree = mean_absolute_error(y_val, y_pred_tree)\n",
    "print(f'Mean Absolute Error (Decision Tree): {mae_tree}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05e6cea-08c6-46dd-9374-f0d19994c88c",
   "metadata": {},
   "source": [
    "## Evaluation of the Decision Tree Model\n",
    "\n",
    "We trained a **Decision Tree** model, and the resulting **Mean Absolute Error (MAE)** was approximately **27,587**. This is higher than the MAE from the Linear Regression model, which indicates that the decision tree, in its current form, is not performing as well.\n",
    "\n",
    "### Why Did the Decision Tree Perform Worse?\n",
    "\n",
    "1. **Overfitting**: \n",
    "   - Decision Trees can easily overfit to the training data, especially if they are allowed to grow too deep. This might lead to excellent performance on the training data but poor generalization to unseen data (like the validation set).\n",
    "\n",
    "2. **Default Hyperparameters**:\n",
    "   - By default, the decision tree may have created a very complex tree, capturing noise and not just the actual patterns in the data. Without any constraints (e.g., limiting tree depth), this can lead to high variance.\n",
    "\n",
    "### Next Step: Tuning the Decision Tree\n",
    "\n",
    "To improve the performance of the Decision Tree, we need to **tune its hyperparameters**. Some important hyperparameters to tune include:\n",
    "- **max_depth**: Limits the depth of the tree to prevent overfitting.\n",
    "- **min_samples_split**: The minimum number of samples required to split a node.\n",
    "- **min_samples_leaf**: The minimum number of samples that a leaf node must have.\n",
    "\n",
    "We will tune the **max_depth** hyperparameter first to see if limiting the depth of the tree can reduce overfitting and improve the MAE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16fd6a8c-96a1-465a-9569-534b1cf28526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (Tuned Decision Tree): 27511.28283135003\n"
     ]
    }
   ],
   "source": [
    "# Tune the max_depth hyperparameter of the Decision Tree\n",
    "tree_model_tuned = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "\n",
    "# Train the tuned model on the training data\n",
    "tree_model_tuned.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred_tree_tuned = tree_model_tuned.predict(X_val)\n",
    "\n",
    "# Calculate the Mean Absolute Error (MAE) for the tuned model\n",
    "mae_tree_tuned = mean_absolute_error(y_val, y_pred_tree_tuned)\n",
    "print(f'Mean Absolute Error (Tuned Decision Tree): {mae_tree_tuned}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ef762-a861-450c-a578-9a0d8eaaafdd",
   "metadata": {},
   "source": [
    "## Decision Tree Tuning Results\n",
    "\n",
    "After tuning the **max_depth** hyperparameter of the Decision Tree, the **Mean Absolute Error (MAE)** only slightly improved, from **27,587** to **27,511**. This minor improvement suggests that tuning the depth of the tree is not sufficient to significantly enhance the model's performance.\n",
    "\n",
    "### Evaluation of the Situation\n",
    "\n",
    "1. **Decision Trees**:\n",
    "   - Even after tuning, the Decision Tree model does not outperform the Linear Regression model, which had an MAE of **21,109**. This suggests that the relationships in the data might be simpler and more linear in nature, which is why a Decision Tree model (which is more complex and captures non-linear relationships) is not yielding better results.\n",
    "\n",
    "2. **Data Simplicity**:\n",
    "   - It’s possible that the dataset’s features have a more linear relationship with the target (`SalePrice`), meaning that **Linear Regression** might be better suited for this task.\n",
    "   - For instance, house prices may increase in a relatively straightforward way as features like living area or overall quality increase. Complex algorithms that model non-linear interactions (like Decision Trees) may not be necessary here.\n",
    "\n",
    "### Next Steps: Trying Different Algorithms\n",
    "\n",
    "Given that **Linear Regression** outperformed the Decision Tree, it might be worth trying a few other algorithms to explore different aspects of the data:\n",
    "1. **Random Forests or Gradient Boosting**:\n",
    "   - These ensemble methods can help improve upon Decision Trees by combining multiple trees and reducing overfitting.\n",
    "2. **Neural Networks**:\n",
    "   - Neural networks might capture more complex patterns in the data, but given the dataset size and the relatively straightforward nature of the problem, neural networks may not provide significant benefits.\n",
    "3. **Regularized Linear Models**:\n",
    "   - Using models like **Ridge Regression** or **Lasso Regression**, which add regularization to the linear model, can help improve performance by penalizing large coefficients and reducing overfitting.\n",
    "\n",
    "### Conclusion: Is Linear Regression Better?\n",
    "\n",
    "In this case, it seems that the relationships in the dataset might be simple enough that **Linear Regression** works better than more complex models like Decision Trees. While it's always worth experimenting with different algorithms, the linear relationship in this dataset might be driving better results with simpler models.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Step: Trying Random Forest\n",
    "\n",
    "Let's next try a **Random Forest** model, which is an ensemble method that builds multiple Decision Trees and averages their predictions to improve performance and reduce overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45a5886f-c72c-4d4a-97be-00093574a98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (Random Forest): 17615.47934931507\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "forest_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred_forest = forest_model.predict(X_val)\n",
    "\n",
    "# Calculate the Mean Absolute Error (MAE)\n",
    "mae_forest = mean_absolute_error(y_val, y_pred_forest)\n",
    "print(f'Mean Absolute Error (Random Forest): {mae_forest}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909412be-6a46-49e1-b8e1-33c91a54c2f5",
   "metadata": {},
   "source": [
    "## Random Forest Results\n",
    "\n",
    "The **Random Forest** model resulted in a **Mean Absolute Error (MAE)** of **17,615**, which is a significant improvement over both the Decision Tree (MAE ~27,500) and Linear Regression (MAE ~21,100). This shows that Random Forest is able to capture more complex patterns in the data compared to the simpler models.\n",
    "\n",
    "### What Does This Result Show?\n",
    "\n",
    "1. **Improved Performance**:\n",
    "   - The Random Forest's MAE of **17,615** indicates that it is better at capturing the relationships in the data compared to both Linear Regression and Decision Trees. This makes sense because Random Forests, as an ensemble method, aggregate multiple decision trees, reducing the risk of overfitting while still capturing complex patterns.\n",
    "\n",
    "2. **Random Forest's Strengths**:\n",
    "   - Random Forests combine the strengths of many decision trees, averaging their predictions to reduce variance. This helps the model generalize better to unseen data, as evidenced by the lower error.\n",
    "   - It also handles both linear and non-linear relationships well, making it a versatile model for this dataset.\n",
    "\n",
    "### How Do We Proceed?\n",
    "\n",
    "Since Random Forest has shown significant improvement, we can now focus on **fine-tuning** the model to optimize its performance even further. Some common hyperparameters to tune in a Random Forest include:\n",
    "- **n_estimators**: The number of trees in the forest. More trees generally improve performance but also increase computation time.\n",
    "- **max_depth**: The maximum depth of each tree. Limiting the depth can help prevent overfitting.\n",
    "- **min_samples_split** and **min_samples_leaf**: These control when nodes are split and how many samples a leaf must have. Higher values can reduce overfitting.\n",
    "\n",
    "Alternatively, if we want to explore further improvements, we could try other ensemble methods like **Gradient Boosting**, which builds trees sequentially and corrects mistakes from previous trees.\n",
    "\n",
    "### Conclusion:\n",
    "The Random Forest model currently provides the best performance with an MAE of **17,615**. The next step would be to either:\n",
    "1. **Tune the Random Forest hyperparameters** to further optimize the model.\n",
    "2. **Try Gradient Boosting**, which might provide even better results by focusing on correcting errors in predictions step by step.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Step: Hyperparameter Tuning for Random Forest\n",
    "\n",
    "Let's start by tuning some key hyperparameters for the Random Forest model and see if we can further improve the MAE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b28e4e74-876d-4e1f-8a14-1b9d04ff687d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (Tuned Random Forest): 17461.31504498976\n"
     ]
    }
   ],
   "source": [
    "# Tune the hyperparameters of the Random Forest\n",
    "forest_model_tuned = RandomForestRegressor(n_estimators=300, max_depth=20, random_state=42)\n",
    "\n",
    "# Train the tuned model\n",
    "forest_model_tuned.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred_forest_tuned = forest_model_tuned.predict(X_val)\n",
    "\n",
    "# Calculate the Mean Absolute Error (MAE) for the tuned model\n",
    "mae_forest_tuned = mean_absolute_error(y_val, y_pred_forest_tuned)\n",
    "print(f'Mean Absolute Error (Tuned Random Forest): {mae_forest_tuned}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42df00b2-e588-4618-b879-97e6e66d24af",
   "metadata": {},
   "source": [
    "## Explanation of Hyperparameters in Random Forest\n",
    "\n",
    "After tuning the **max_depth** to 20 and increasing the **n_estimators** (iterations) to 400, you achieved an MAE of **17,400**, which is a slight improvement. Let’s break down what these hyperparameters mean and why adjusting them can affect the model’s performance.\n",
    "\n",
    "### Key Hyperparameters:\n",
    "\n",
    "1. **max_depth**:\n",
    "   - **Definition**: This parameter controls the maximum depth of each tree in the forest. A deeper tree can model more complex relationships but also increases the risk of overfitting, where the model performs well on training data but poorly on unseen data.\n",
    "   - **Effect**: Increasing the `max_depth` allows the model to split the data more and make finer decisions, which can improve accuracy up to a certain point. Beyond that, it might overfit.\n",
    "   - In your case, setting `max_depth=20` gave a good balance between capturing complexity and avoiding overfitting.\n",
    "\n",
    "2. **n_estimators**:\n",
    "   - **Definition**: This parameter controls the number of trees in the Random Forest. More trees generally lead to better performance, as the model averages predictions over more trees, reducing variance and improving generalization.\n",
    "   - **Effect**: Increasing `n_estimators` makes the model more robust, but with diminishing returns as it increases computation time. In your case, increasing to 400 iterations led to a slight improvement in performance.\n",
    "\n",
    "### Next Step: Hyperparameter Tuning Loop\n",
    "\n",
    "To find the optimal combination of `max_depth` and `n_estimators`, we can set up a loop that systematically varies both parameters and tracks the resulting MAE. This will allow us to identify the combination that works best for this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f395f949-d4fb-4949-af6d-6ea285d7041b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth: 25, n_estimators: 400, MAE: 17456.371685216895\n",
      "max_depth: 25, n_estimators: 500, MAE: 17459.093170091324\n",
      "max_depth: 20, n_estimators: 400, MAE: 17461.181618514885\n",
      "max_depth: 20, n_estimators: 300, MAE: 17461.31504498976\n",
      "max_depth: 15, n_estimators: 400, MAE: 17463.258091350883\n",
      "max_depth: 15, n_estimators: 500, MAE: 17468.437681632855\n",
      "max_depth: 25, n_estimators: 300, MAE: 17469.458280060884\n",
      "max_depth: 20, n_estimators: 500, MAE: 17480.510176232194\n",
      "max_depth: 15, n_estimators: 300, MAE: 17503.25418301735\n",
      "max_depth: 20, n_estimators: 200, MAE: 17514.76008789611\n",
      "max_depth: 15, n_estimators: 200, MAE: 17524.09608539495\n",
      "max_depth: 25, n_estimators: 200, MAE: 17531.990650684933\n",
      "max_depth: 20, n_estimators: 100, MAE: 17600.040353847635\n",
      "max_depth: 25, n_estimators: 100, MAE: 17612.309760273973\n",
      "max_depth: 15, n_estimators: 100, MAE: 17658.094402714447\n",
      "max_depth: 10, n_estimators: 400, MAE: 17712.123787781504\n",
      "max_depth: 10, n_estimators: 500, MAE: 17727.101839952917\n",
      "max_depth: 10, n_estimators: 300, MAE: 17733.995256870385\n",
      "max_depth: 10, n_estimators: 200, MAE: 17736.336643965667\n",
      "max_depth: 10, n_estimators: 100, MAE: 17831.797280043196\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Define a range of values for max_depth and n_estimators\n",
    "max_depth_range = [10, 15, 20, 25]\n",
    "n_estimators_range = [100, 200, 300, 400, 500]\n",
    "\n",
    "# To store the results\n",
    "results = []\n",
    "\n",
    "# Loop through all combinations of max_depth and n_estimators\n",
    "for max_depth in max_depth_range:\n",
    "    for n_estimators in n_estimators_range:\n",
    "        # Initialize the Random Forest with current parameters\n",
    "        forest_model = RandomForestRegressor(max_depth=max_depth, n_estimators=n_estimators, random_state=42)\n",
    "        \n",
    "        # Train the model\n",
    "        forest_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions on the validation set\n",
    "        y_pred = forest_model.predict(X_val)\n",
    "        \n",
    "        # Calculate the MAE\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        \n",
    "        # Append the results as a tuple (max_depth, n_estimators, mae)\n",
    "        results.append((max_depth, n_estimators, mae))\n",
    "\n",
    "# Convert results to a sorted list and display the top combinations\n",
    "results = sorted(results, key=lambda x: x[2])\n",
    "for result in results:\n",
    "    print(f\"max_depth: {result[0]}, n_estimators: {result[1]}, MAE: {result[2]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555ab3c1-ff07-4c76-9963-e009a469b760",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Results for Random Forest\n",
    "\n",
    "After running a loop to tune the hyperparameters (`max_depth` and `n_estimators`), we identified the best combinations. Here are the top-performing models based on **Mean Absolute Error (MAE)**:\n",
    "\n",
    "### Top Results:\n",
    "1. **max_depth: 25, n_estimators: 400**, MAE: **17,456**\n",
    "2. **max_depth: 25, n_estimators: 500**, MAE: **17,459**\n",
    "3. **max_depth: 20, n_estimators: 400**, MAE: **17,461**\n",
    "4. **max_depth: 20, n_estimators: 300**, MAE: **17,461**\n",
    "5. **max_depth: 15, n_estimators: 400**, MAE: **17,463**\n",
    "\n",
    "### Interpretation:\n",
    "1. **Optimal Depth**: It appears that deeper trees (with `max_depth` of 20-25) are better at capturing the complexity of the data. Shallow trees (e.g., `max_depth: 10`) resulted in higher MAEs.\n",
    "2. **Number of Trees**: Increasing the number of trees (`n_estimators`) from 300 to 500 did not yield a significant improvement beyond 400 trees. More trees generally help, but after a certain point, the improvement diminishes.\n",
    "\n",
    "### Conclusion:\n",
    "- The best model uses **max_depth: 25** and **n_estimators: 400**, resulting in an MAE of **17,456**. This is a strong improvement compared to previous models like Linear Regression (~21,100) and Decision Trees (~27,500).\n",
    "- Increasing the `max_depth` seems to help, but additional trees beyond 400 don't provide much improvement, indicating that 400 trees is sufficient for this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f761eb-2ce6-4bf2-b689-8f59a7b22259",
   "metadata": {},
   "source": [
    "## Trying Another Model: Gradient Boosting\n",
    "\n",
    "Since Random Forest provided good results but still has room for improvement, the next step is to try **Gradient Boosting**. Gradient Boosting is another powerful ensemble method that builds trees sequentially, where each tree tries to correct the errors of the previous one. This makes Gradient Boosting more effective at handling complex patterns than Random Forests, as it focuses on reducing bias over multiple iterations.\n",
    "\n",
    "### Why Gradient Boosting?\n",
    "- **Sequential learning**: Unlike Random Forests, which build trees independently, Gradient Boosting builds trees in a sequential manner, where each tree attempts to correct the mistakes of the previous trees.\n",
    "- **More control over overfitting**: Gradient Boosting tends to perform better with hyperparameter tuning, offering finer control over the learning process and tree complexity.\n",
    "\n",
    "### Let's try Gradient Boosting using the `GradientBoostingRegressor` from `sklearn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97a0d1f7-d17b-4e66-9025-d5330421937a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (Gradient Boosting): 17205.71765440213\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Initialize the Gradient Boosting model\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred_gb = gb_model.predict(X_val)\n",
    "\n",
    "# Calculate the Mean Absolute Error (MAE)\n",
    "mae_gb = mean_absolute_error(y_val, y_pred_gb)\n",
    "print(f'Mean Absolute Error (Gradient Boosting): {mae_gb}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37422464-d624-47a3-862e-5f9545f973d3",
   "metadata": {},
   "source": [
    "## Gradient Boosting Results\n",
    "\n",
    "The **Gradient Boosting** model resulted in a **Mean Absolute Error (MAE)** of **17,205**, which is even better than the tuned Random Forest model (MAE ~17,456). This suggests that Gradient Boosting is more effective at capturing the patterns in this dataset.\n",
    "\n",
    "### Why is Gradient Boosting Performing Better?\n",
    "- **Sequential Learning**: Gradient Boosting corrects mistakes made by previous models, leading to a more refined set of predictions.\n",
    "- **Bias Reduction**: The model reduces bias by sequentially learning from residuals, leading to more accurate predictions.\n",
    "\n",
    "### Hyperparameter Tuning for Gradient Boosting\n",
    "\n",
    "Gradient Boosting has several hyperparameters that can significantly affect its performance. Some key parameters to tune include:\n",
    "- **n_estimators**: The number of boosting stages or trees. More trees usually improve performance but increase computation time.\n",
    "- **learning_rate**: This controls how much each tree corrects the errors of the previous one. Lower values can improve performance, but you need more trees.\n",
    "- **max_depth**: The maximum depth of each tree. Deeper trees can model more complex patterns but are prone to overfitting.\n",
    "- **subsample**: The fraction of samples used for fitting each individual tree. Smaller values can improve generalization.\n",
    "\n",
    "### Next Step: Tuning the Hyperparameters\n",
    "\n",
    "We can use a loop to test different combinations of `n_estimators`, `learning_rate`, and `max_depth` to see which combination yields the best results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "230537a5-08f9-405d-ac11-e5bb2e5eb721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 400, learning_rate: 0.05, max_depth: 5, MAE: 15972.977296066885\n",
      "n_estimators: 300, learning_rate: 0.05, max_depth: 5, MAE: 15994.068067639964\n",
      "n_estimators: 200, learning_rate: 0.1, max_depth: 5, MAE: 16001.807618517276\n",
      "n_estimators: 100, learning_rate: 0.1, max_depth: 5, MAE: 16026.443816349469\n",
      "n_estimators: 300, learning_rate: 0.1, max_depth: 5, MAE: 16030.627509743945\n"
     ]
    }
   ],
   "source": [
    "# Define a range of values for n_estimators, learning_rate, and max_depth\n",
    "n_estimators_range = [100, 200, 300, 400]\n",
    "learning_rate_range = [0.01, 0.05, 0.1]\n",
    "max_depth_range = [3, 5, 7]\n",
    "\n",
    "# To store the results\n",
    "results_gb = []\n",
    "\n",
    "# Loop through all combinations of n_estimators, learning_rate, and max_depth\n",
    "for n_estimators in n_estimators_range:\n",
    "    for learning_rate in learning_rate_range:\n",
    "        for max_depth in max_depth_range:\n",
    "            # Initialize the Gradient Boosting model with current parameters\n",
    "            gb_model_tuned = GradientBoostingRegressor(n_estimators=n_estimators, \n",
    "                                                       learning_rate=learning_rate, \n",
    "                                                       max_depth=max_depth, \n",
    "                                                       random_state=42)\n",
    "            \n",
    "            # Train the model\n",
    "            gb_model_tuned.fit(X_train, y_train)\n",
    "            \n",
    "            # Make predictions on the validation set\n",
    "            y_pred_gb_tuned = gb_model_tuned.predict(X_val)\n",
    "            \n",
    "            # Calculate the MAE\n",
    "            mae_gb_tuned = mean_absolute_error(y_val, y_pred_gb_tuned)\n",
    "            \n",
    "            # Append the results as a tuple (n_estimators, learning_rate, max_depth, mae)\n",
    "            results_gb.append((n_estimators, learning_rate, max_depth, mae_gb_tuned))\n",
    "\n",
    "# Sort the results and display the top combinations\n",
    "results_gb = sorted(results_gb, key=lambda x: x[3])\n",
    "for result in results_gb[:5]:  # Display top 5\n",
    "    print(f\"n_estimators: {result[0]}, learning_rate: {result[1]}, max_depth: {result[2]}, MAE: {result[3]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ab468-f557-484f-80b7-444c49ed395b",
   "metadata": {},
   "source": [
    "## Final Gradient Boosting Results\n",
    "\n",
    "The tuning loop for **Gradient Boosting** produced the best result with:\n",
    "- **n_estimators: 400**\n",
    "- **learning_rate: 0.05**\n",
    "- **max_depth: 5**\n",
    "- **MAE: 15,973**\n",
    "\n",
    "This is a significant improvement from the previous models:\n",
    "- **Linear Regression MAE**: ~21,109\n",
    "- **Tuned Random Forest MAE**: ~17,456\n",
    "- **Gradient Boosting Initial MAE**: ~17,205\n",
    "\n",
    "### Is It Worth Trying Other Models?\n",
    "\n",
    "Given the current results, **Gradient Boosting** has shown excellent performance and may already be a good candidate for a production model. While other algorithms like **XGBoost** or **LightGBM** may offer marginal improvements, they often work similarly to Gradient Boosting and might not yield drastically better results on this dataset.\n",
    "\n",
    "### Is This Project Workable in Real Life?\n",
    "\n",
    "Absolutely! Here's why:\n",
    "1. **Structured Data Problem**: Predicting house prices based on structured data is a common real-life application in fields like real estate or financial services.\n",
    "2. **Machine Learning Engineer Role**: As a machine learning engineer, being able to develop, tune, and evaluate models like Random Forest and Gradient Boosting is essential. This project demonstrates key skills, including data preprocessing, feature engineering, and hyperparameter tuning.\n",
    "3. **Deployability**: The models you’ve built can easily be deployed in real-world applications using APIs or integrated into data pipelines for continuous predictions.\n",
    "4. **Explainability**: Gradient Boosting and Random Forest models are interpretable, meaning you can explain which features impact the predictions the most, which is crucial in business contexts.\n",
    "\n",
    "### Would Neural Networks Help?\n",
    "\n",
    "While neural networks are powerful, they generally excel in cases where there are highly complex patterns in data (e.g., images, text, deep relationships between features). In this case:\n",
    "- **Structured Data**: For tabular data like this, algorithms such as **Gradient Boosting** or **Random Forest** usually perform better or on par with neural networks.\n",
    "- **Computational Cost**: Neural networks require more computational resources and tuning to achieve similar results to the Gradient Boosting model you already tuned.\n",
    "\n",
    "Given the results from Gradient Boosting, neural networks would likely provide **marginal benefits at a much higher cost**. So, it may not be worth trying neural networks for this particular dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e542af-6a8c-4e18-87d9-76b6a38788a3",
   "metadata": {},
   "source": [
    "## Trying a Neural Network Model\n",
    "\n",
    "Even though **Gradient Boosting** has proven to be highly effective for this dataset, it’s worth experimenting with a **Neural Network** to see how it performs. Neural networks are powerful, but for structured data like this, they often don’t outperform models like Gradient Boosting. However, for curiosity’s sake, let’s try a basic neural network using **Keras** and see how it compares.\n",
    "\n",
    "### Key Considerations for Neural Networks:\n",
    "- **Feature Scaling**: Neural networks typically require that features are scaled. We will apply **StandardScaler** to ensure all features are on the same scale.\n",
    "- **Architecture**: We'll create a simple feed-forward neural network with a few dense layers.\n",
    "\n",
    "Let's start by setting up a neural network and training it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39a6dbb6-14c9-4e18-8867-8aede134af1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 176285.2500 - val_loss: 178820.6562\n",
      "Epoch 2/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781us/step - loss: 182358.8906 - val_loss: 178588.2812\n",
      "Epoch 3/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 179599.4844 - val_loss: 177004.2500\n",
      "Epoch 4/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 177299.9219 - val_loss: 170712.7188\n",
      "Epoch 5/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 170505.3750 - val_loss: 153472.9688\n",
      "Epoch 6/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 147293.9688 - val_loss: 118456.4219\n",
      "Epoch 7/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 112232.0469 - val_loss: 68946.2266\n",
      "Epoch 8/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 66041.4219 - val_loss: 44917.0234\n",
      "Epoch 9/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 45071.8945 - val_loss: 39321.6641\n",
      "Epoch 10/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 39992.6680 - val_loss: 35376.7422\n",
      "Epoch 11/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 37039.6758 - val_loss: 33358.0039\n",
      "Epoch 12/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 31532.7773 - val_loss: 32169.2676\n",
      "Epoch 13/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 30853.4180 - val_loss: 31227.4453\n",
      "Epoch 14/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 27803.5273 - val_loss: 30335.5371\n",
      "Epoch 15/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 26865.8828 - val_loss: 29648.6582\n",
      "Epoch 16/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 26146.2441 - val_loss: 28890.8730\n",
      "Epoch 17/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 24245.0410 - val_loss: 28324.4023\n",
      "Epoch 18/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 23894.1875 - val_loss: 27895.6191\n",
      "Epoch 19/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 22669.1855 - val_loss: 27690.6895\n",
      "Epoch 20/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 22963.0938 - val_loss: 27138.3262\n",
      "Epoch 21/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 22306.3457 - val_loss: 26491.8008\n",
      "Epoch 22/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 20568.8281 - val_loss: 26019.1523\n",
      "Epoch 23/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 20457.1816 - val_loss: 25668.9707\n",
      "Epoch 24/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 19999.5918 - val_loss: 25412.1250\n",
      "Epoch 25/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 20128.9863 - val_loss: 25056.4277\n",
      "Epoch 26/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 19123.3926 - val_loss: 24669.1660\n",
      "Epoch 27/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 17648.3281 - val_loss: 24460.2012\n",
      "Epoch 28/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 17504.7461 - val_loss: 24211.5215\n",
      "Epoch 29/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 18156.3477 - val_loss: 24242.9609\n",
      "Epoch 30/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 17754.3555 - val_loss: 23842.9141\n",
      "Epoch 31/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 17164.9199 - val_loss: 24184.3008\n",
      "Epoch 32/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 18019.8164 - val_loss: 23611.5547\n",
      "Epoch 33/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 16088.3789 - val_loss: 23517.1484\n",
      "Epoch 34/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 16510.9629 - val_loss: 23283.9102\n",
      "Epoch 35/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 15075.1582 - val_loss: 23155.3262\n",
      "Epoch 36/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 14839.7598 - val_loss: 23107.6230\n",
      "Epoch 37/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 15213.3174 - val_loss: 23305.6641\n",
      "Epoch 38/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 14248.0068 - val_loss: 23080.3867\n",
      "Epoch 39/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 14526.0273 - val_loss: 22865.6797\n",
      "Epoch 40/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 14274.7158 - val_loss: 22889.0000\n",
      "Epoch 41/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 13683.0459 - val_loss: 22746.8750\n",
      "Epoch 42/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 13776.8594 - val_loss: 22713.6387\n",
      "Epoch 43/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 13771.2324 - val_loss: 22445.3105\n",
      "Epoch 44/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 13174.3770 - val_loss: 22290.7520\n",
      "Epoch 45/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 12694.5098 - val_loss: 22330.9355\n",
      "Epoch 46/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 13393.4326 - val_loss: 22227.1172\n",
      "Epoch 47/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - loss: 12840.1836 - val_loss: 22100.1328\n",
      "Epoch 48/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 12693.1650 - val_loss: 22091.2422\n",
      "Epoch 49/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 11956.7861 - val_loss: 22105.1211\n",
      "Epoch 50/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 11907.4844 - val_loss: 21904.3828\n",
      "Epoch 51/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 11770.3447 - val_loss: 21946.7129\n",
      "Epoch 52/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 12677.3945 - val_loss: 21771.3770\n",
      "Epoch 53/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 11626.0723 - val_loss: 21843.3145\n",
      "Epoch 54/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 12383.6904 - val_loss: 21732.0117\n",
      "Epoch 55/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 11346.0732 - val_loss: 21963.9023\n",
      "Epoch 56/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 11840.3242 - val_loss: 21765.6230\n",
      "Epoch 57/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 12004.7764 - val_loss: 21645.6328\n",
      "Epoch 58/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 11396.0625 - val_loss: 21555.8027\n",
      "Epoch 59/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 11438.0742 - val_loss: 21507.0684\n",
      "Epoch 60/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 12184.5439 - val_loss: 21443.8672\n",
      "Epoch 61/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 10340.6953 - val_loss: 21359.6738\n",
      "Epoch 62/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 10840.5762 - val_loss: 21175.2031\n",
      "Epoch 63/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 11322.7803 - val_loss: 21361.0723\n",
      "Epoch 64/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 10596.7314 - val_loss: 21083.0801\n",
      "Epoch 65/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 10699.6670 - val_loss: 21108.3828\n",
      "Epoch 66/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 10431.8398 - val_loss: 21105.7539\n",
      "Epoch 67/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 10649.6387 - val_loss: 21125.2090\n",
      "Epoch 68/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 10100.7559 - val_loss: 20964.9961\n",
      "Epoch 69/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 9868.1113 - val_loss: 21069.8340\n",
      "Epoch 70/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 9850.4307 - val_loss: 20951.8535\n",
      "Epoch 71/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 894us/step - loss: 10190.5771 - val_loss: 20897.8652\n",
      "Epoch 72/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 9279.3652 - val_loss: 20847.0020\n",
      "Epoch 73/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 10253.2227 - val_loss: 20763.2188\n",
      "Epoch 74/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 9653.7109 - val_loss: 20818.3496\n",
      "Epoch 75/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 8958.4326 - val_loss: 20782.5488\n",
      "Epoch 76/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 9733.1123 - val_loss: 20742.1582\n",
      "Epoch 77/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 9673.9629 - val_loss: 20616.3223\n",
      "Epoch 78/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 9487.2988 - val_loss: 20571.4980\n",
      "Epoch 79/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 8819.7764 - val_loss: 20600.1660\n",
      "Epoch 80/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 8720.6768 - val_loss: 20448.5488\n",
      "Epoch 81/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 9140.2734 - val_loss: 20540.2461\n",
      "Epoch 82/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 9376.0781 - val_loss: 20409.4102\n",
      "Epoch 83/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 9002.1230 - val_loss: 20422.9004\n",
      "Epoch 84/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 9770.6562 - val_loss: 20505.1074\n",
      "Epoch 85/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 900us/step - loss: 10128.2617 - val_loss: 20370.0312\n",
      "Epoch 86/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 8973.3682 - val_loss: 20185.2129\n",
      "Epoch 87/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 8802.3066 - val_loss: 20309.4629\n",
      "Epoch 88/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 8843.4854 - val_loss: 20176.9961\n",
      "Epoch 89/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 8468.1240 - val_loss: 20184.8633\n",
      "Epoch 90/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 8650.5293 - val_loss: 20136.8438\n",
      "Epoch 91/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - loss: 8471.0762 - val_loss: 20129.2578\n",
      "Epoch 92/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 7755.6108 - val_loss: 20051.2988\n",
      "Epoch 93/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 8494.6689 - val_loss: 20047.0586\n",
      "Epoch 94/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 8390.8984 - val_loss: 20011.8672\n",
      "Epoch 95/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 7863.4224 - val_loss: 20053.9219\n",
      "Epoch 96/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 7522.2339 - val_loss: 19886.1074\n",
      "Epoch 97/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 8069.4907 - val_loss: 19934.1152\n",
      "Epoch 98/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 8144.3608 - val_loss: 19895.6621\n",
      "Epoch 99/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 7889.4595 - val_loss: 19948.9277\n",
      "Epoch 100/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 8828.8232 - val_loss: 19865.7656\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Mean Absolute Error (Neural Network): 19865.76494274401\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Step 1: Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Step 2: Define the neural network architecture using Input layer\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer and one hidden layer\n",
    "model.add(Input(shape=(X_train_scaled.shape[1],)))  # Using Input layer, shape should match feature count\n",
    "model.add(Dense(128, activation='relu'))  # 128 neurons, hidden layer\n",
    "model.add(Dense(64, activation='relu'))  # 64 neurons, hidden layer\n",
    "model.add(Dense(32, activation='relu'))  # 32 neurons, hidden layer\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))  # Output for regression task\n",
    "\n",
    "# Step 3: Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
    "\n",
    "# Step 4: Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_data=(X_val_scaled, y_val), verbose=1)\n",
    "\n",
    "# Step 5: Make predictions on the validation set\n",
    "y_pred_nn = model.predict(X_val_scaled)\n",
    "\n",
    "# Step 6: Calculate the Mean Absolute Error\n",
    "mae_nn = mean_absolute_error(y_val, y_pred_nn)\n",
    "print(f'Mean Absolute Error (Neural Network): {mae_nn}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a894fdd1-f587-4e19-b5e6-38793b002ffd",
   "metadata": {},
   "source": [
    "## Neural Network Results\n",
    "\n",
    "After training the Neural Network model, the **Mean Absolute Error (MAE)** was approximately **19,961**. While this is a respectable result, it did not outperform the best results from **Gradient Boosting** (MAE ~15,973) or **Random Forest** (MAE ~17,456).\n",
    "\n",
    "### Key Observations:\n",
    "1. **Neural Networks Performance**:\n",
    "   - Despite fine-tuning the architecture (hidden layers, neurons) and training for 100 epochs, the Neural Network’s performance was slightly worse than tree-based models like Gradient Boosting.\n",
    "   - This is expected, as **Neural Networks** often perform best on highly complex data (like images, text, or large-scale data) rather than structured data like this housing dataset.\n",
    "\n",
    "2. **Gradient Boosting Is the Best Performer**:\n",
    "   - For structured/tabular data like this, **Gradient Boosting** algorithms often outperform neural networks due to their ability to capture complex, non-linear interactions in the data without extensive feature engineering.\n",
    "\n",
    "### Conclusion: Best Algorithm for This Project\n",
    "- **Gradient Boosting** emerged as the best model with an MAE of **15,973**, making it the most suitable for this problem.\n",
    "- **Random Forest** also performed well, but Gradient Boosting’s sequential learning process allowed it to refine predictions more effectively.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps: Moving to a New Project\n",
    "\n",
    "### Where to Go from Here:\n",
    "\n",
    "1. **More Complex Datasets**:\n",
    "   - You can move on to larger, more complex datasets, possibly from industries like **finance**, **healthcare**, or **e-commerce**. These often require handling more features and complex relationships.\n",
    "   \n",
    "2. **Time Series or Sequential Data**:\n",
    "   - You could explore projects that involve **time series forecasting** (predicting future values) or **sequence data** (such as stock prices, weather, or user behavior).\n",
    "\n",
    "3. **Unsupervised Learning**:\n",
    "   - Work on **clustering** or **dimensionality reduction** projects using methods like **K-means** or **PCA** to explore datasets without labeled outcomes.\n",
    "\n",
    "4. **End-to-End Deployment**:\n",
    "   - Deploy one of your models into a real-world application. This could involve using **Flask**, **FastAPI**, or deploying it on platforms like **AWS Lambda** or **Google Cloud**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d1685-d8bf-437c-bf3f-265a2547da9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
